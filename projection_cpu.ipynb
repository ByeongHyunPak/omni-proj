{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def pil_to_tensor(pil_image):\n",
    "  transform = transforms.ToTensor()\n",
    "  tensor_image = transform(pil_image)\n",
    "  return tensor_image\n",
    "\n",
    "def tensor_to_pil(tensor_image):\n",
    "    transform = transforms.ToPILImage()\n",
    "    pil_image = transform(tensor_image)\n",
    "    return pil_image\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "  images = []\n",
    "  for filename in os.listdir(folder):\n",
    "    img_path = os.path.join(folder, filename)\n",
    "    if os.path.isfile(img_path) and img_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "      try:\n",
    "        img = Image.open(img_path)\n",
    "        images.append(img)\n",
    "      except Exception as e:\n",
    "        print(f\"Error loading image {img_path}: {e}\")\n",
    "  return images\n",
    "\n",
    "def draw_boundary_on_image(image_tensor, mask_tensor, boundary_color=(0, 0, 255), boundary_thickness=10):\n",
    "    if mask_tensor.dtype != torch.uint8:\n",
    "        mask_tensor = (mask_tensor * 255).to(torch.uint8)\n",
    "\n",
    "    if image_tensor.dtype != torch.uint8:\n",
    "        image_tensor = (image_tensor * 255).to(torch.uint8)\n",
    "\n",
    "    if len(mask_tensor.shape) == 3:\n",
    "        mask_tensor = mask_tensor[0, :, :]\n",
    "\n",
    "    mask_np = mask_tensor.numpy().astype(np.uint8)\n",
    "    contours, _ = cv2.findContours(mask_np, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    image_np = image_tensor.permute(1, 2, 0).numpy().copy()  # [C, H, W] -> [H, W, C]\n",
    "    cv2.drawContours(image_np, contours, -1, boundary_color, boundary_thickness)\n",
    "    output_image_tensor = torch.from_numpy(image_np).permute(2, 0, 1)  # 다시 [C, H, W]로 변경\n",
    "    \n",
    "    return output_image_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ByeongHyunPak/omni-proj.git\n",
    "\n",
    "import os\n",
    "os.chdir('/content/omni-proj/omni-proj')\n",
    "\n",
    "imgs_folder = '/content/omni-proj/imgs/erps' \n",
    "images = load_images_from_folder(imgs_folder)\n",
    "\n",
    "for img in images:\n",
    "    img = pil_to_tensor(img)\n",
    "    print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from utils.projection_cpu import gridy2x_erp2per, gridy2x_per2erp, gridy2x_erp2fis, gridy2x_fis2erp\n",
    "\n",
    "def erp2per(hr_erp_img, THETA, PHI, FOVy=75, FOVx=360, HWy=(512, 512)):\n",
    "    hr_erp_img = pil_to_tensor(hr_erp_img)\n",
    "    HWx = hr_erp_img.shape[-2:]\n",
    "\n",
    "    gridy = utils.make_coord(HWy)\n",
    "    gridy2x, masky = gridy2x_erp2per(\n",
    "        gridy, HWy, HWx, THETA, PHI, FOVy, FOVx)\n",
    "    gridy2x = gridy2x.view(*HWy, 2)\n",
    "\n",
    "    inp = F.grid_sample(hr_erp_img.unsqueeze(0),\n",
    "                        gridy2x.unsqueeze(0).flip(-1),\n",
    "                        mode='bicubic',\n",
    "                        padding_mode='reflection',\n",
    "                        align_corners=False).clamp_(0, 1)[0]\n",
    "\n",
    "    gridx = utils.make_coord(HWx, flatten=False)\n",
    "    gridx2y, maskx = gridy2x_per2erp(\n",
    "        gridx, HWx, HWy, THETA, PHI, FOVx, FOVy)\n",
    "    \n",
    "    maskx = maskx.view(1, *HWx)\n",
    "    valid_hr_erp_img = hr_erp_img * maskx\n",
    "    \n",
    "    return inp, valid_hr_erp_img, maskx\n",
    "\n",
    "def per2erp(hr_pers_img, THETA, PHI, FOVy=360, FOVx=75, HWy=(512, 1024)):\n",
    "    hr_pers_img = pil_to_tensor(hr_pers_img)\n",
    "    HWx = hr_pers_img.shape[-2:]\n",
    "\n",
    "    gridy = utils.make_coord(HWy)\n",
    "    gridy2x, masky = gridy2x_per2erp(\n",
    "        gridy, HWy, HWx, THETA, PHI, FOVy, FOVx)\n",
    "    gridy2x = gridy2x.view(*HWy, 2)\n",
    "    masky = masky.view(1, *HWy)\n",
    "\n",
    "    inp = F.grid_sample(hr_pers_img.unsqueeze(0),\n",
    "                        gridy2x.unsqueeze(0).flip(-1),\n",
    "                        mode='bicubic',\n",
    "                        padding_mode='reflection',\n",
    "                        align_corners=False).clamp_(0, 1)[0]\n",
    "    inp = inp * masky\n",
    "\n",
    "    gridx = utils.make_coord(HWx, flatten=False)\n",
    "    gridx2y, maskx = gridy2x_erp2per(\n",
    "        gridx, HWx, HWy, THETA, PHI, FOVx, FOVy)\n",
    "    \n",
    "    maskx = maskx.view(1, *HWx)\n",
    "    valid_hr_pers_img = hr_pers_img * maskx\n",
    "    \n",
    "    return inp, valid_hr_pers_img, maskx\n",
    "\n",
    "\n",
    "def erp2fis(hr_erp_img, THETA, PHI, FOVy=180, FOVx=360, HWy=(1024, 1024)):\n",
    "    hr_erp_img = pil_to_tensor(hr_erp_img)\n",
    "    HWx = hr_erp_img.shape[-2:]\n",
    "\n",
    "    gridy = utils.make_coord(HWy)\n",
    "    gridy2x, masky = gridy2x_erp2fis(\n",
    "        gridy, HWy, HWx, THETA, PHI, FOVy, FOVx)\n",
    "    gridy2x = gridy2x.view(*HWy, 2)\n",
    "    masky = masky.view(1, *HWy)\n",
    "\n",
    "    inp = F.grid_sample(hr_erp_img.unsqueeze(0),\n",
    "                        gridy2x.unsqueeze(0).flip(-1),\n",
    "                        mode='bicubic',\n",
    "                        padding_mode='reflection',\n",
    "                        align_corners=False).clamp_(0, 1)[0]\n",
    "    inp = inp * masky\n",
    "    \n",
    "    gridx = utils.make_coord(HWx, flatten=False)\n",
    "    gridx2y, maskx = gridy2x_fis2erp(\n",
    "        gridx, HWx, HWy, THETA, PHI, FOVx, FOVy)\n",
    "\n",
    "    maskx = maskx.view(1, *HWx)\n",
    "    valid_hr_erp_img = hr_erp_img * maskx\n",
    "\n",
    "    return inp, valid_hr_erp_img, maskx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_erp_img = images[0]\n",
    "display(hr_erp_img)\n",
    "\n",
    "# ERP to Perspective\n",
    "THETA = random.uniform(-135, 135)\n",
    "PHI = random.uniform(-45, 45)\n",
    "\n",
    "pers_img, valid_hr_erp_img, _ = erp2per(hr_erp_img, THETA, PHI, FOVy=90, FOVx=360)\n",
    "\n",
    "display(tensor_to_pil(valid_hr_erp_img))\n",
    "display(tensor_to_pil(pers_img))\n",
    "\n",
    "# Perspective to ERP\n",
    "erp_img, valid_lr_pers_img, _ = per2erp(tensor_to_pil(pers_img), THETA, PHI, FOVy=360, FOVx=90)\n",
    "\n",
    "display(tensor_to_pil(valid_lr_pers_img))\n",
    "display(tensor_to_pil(erp_img))\n",
    "\n",
    "display(tensor_to_pil((pil_to_tensor(hr_erp_img) - erp_img).abs()))\n",
    "display(tensor_to_pil((pil_to_tensor(hr_erp_img) - valid_hr_erp_img).abs()))\n",
    "\n",
    "# ERP to Fisheye\n",
    "THETA = random.uniform(-135, 135)\n",
    "PHI = 0\n",
    "\n",
    "fish_img, valid_hr_erp_img, _ = erp2fis(hr_erp_img, THETA, PHI, FOVy=180, FOVx=360)\n",
    "\n",
    "display(tensor_to_pil(valid_hr_erp_img))\n",
    "display(tensor_to_pil(fish_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ERP to Perspective\n",
    "hr_erp_img = images[3]\n",
    "lr_pers_hw = (512, 512)\n",
    "\n",
    "num_rows = 4\n",
    "num_cols = [3, 6, 6, 3]\n",
    "phi_centers = [-67.5, -22.5, 22.5, 67.5]\n",
    "phi_interval = 180 // num_rows\n",
    "\n",
    "for i, n_cols in enumerate(num_cols):\n",
    "    PHI = phi_centers[i]\n",
    "    global_hr_erp_ten = pil_to_tensor(hr_erp_img)\n",
    "    for j in np.arange(n_cols):\n",
    "        theta_interval = 360 / n_cols\n",
    "        THETA = j * theta_interval + theta_interval / 2\n",
    "        pers_img, valid_hr_erp_img, valid_hr_erp_mask =\\\n",
    "            erp2per(hr_erp_img, THETA, PHI, FOVy=90, FOVx=360, HWy=lr_pers_hw)\n",
    "\n",
    "        hr_erp_ten = pil_to_tensor(hr_erp_img)\n",
    "        hr_erp_ten = draw_boundary_on_image(hr_erp_ten, valid_hr_erp_mask)\n",
    "\n",
    "        random_color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        global_hr_erp_ten = draw_boundary_on_image(global_hr_erp_ten, valid_hr_erp_mask, boundary_color=random_color, boundary_thickness=15)\n",
    "\n",
    "        pers_img = tensor_to_pil(pers_img)\n",
    "        valid_hr_erp_img = tensor_to_pil(valid_hr_erp_img)\n",
    "        valid_hr_erp_mask = tensor_to_pil(valid_hr_erp_mask)\n",
    "        hr_erp_ten = tensor_to_pil(hr_erp_ten)\n",
    "\n",
    "        display(pers_img)\n",
    "        display(valid_hr_erp_img)\n",
    "        display(valid_hr_erp_mask)\n",
    "        display(hr_erp_ten)\n",
    "    display(tensor_to_pil(global_hr_erp_ten))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
