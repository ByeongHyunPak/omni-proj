{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Requiremnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install ninja-build\n",
    "!ninja --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists('/content/nvdiffrast'):\n",
    "  %rm -rf /content/nvdiffrast\n",
    "\n",
    "!git clone --recursive https://github.com/NVlabs/nvdiffrast\n",
    "%cd /content/nvdiffrast\n",
    "!pip install .\n",
    "%cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import nvdiffrast.torch as dr\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Image\n",
    "from torchvision.transforms import ToPILImage, ToTensor\n",
    "from einops import rearrange, reduce, repeat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load OpenGL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_opengl = False # On T4 GPU, only False works, but rasterizer works much better if = True\n",
    "glctx = dr.RasterizeGLContext() if use_opengl else dr.RasterizeCudaContext()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiDiffusion Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
    "\n",
    "def get_views(panorama_height, panorama_width, window_size=64, stride=8):\n",
    "    panorama_height /= 8\n",
    "    panorama_width /= 8\n",
    "    num_blocks_height = (panorama_height - window_size) // stride + 1\n",
    "    num_blocks_width = (panorama_width - window_size) // stride + 1\n",
    "    total_num_blocks = int(num_blocks_height * num_blocks_width)\n",
    "    views = []\n",
    "    for i in range(total_num_blocks):\n",
    "        h_start = int((i // num_blocks_width) * stride)\n",
    "        h_end = h_start + window_size\n",
    "        w_start = int((i % num_blocks_width) * stride)\n",
    "        w_end = w_start + window_size\n",
    "        views.append((h_start, h_end, w_start, w_end))\n",
    "    return views\n",
    "\n",
    "class MultiDiffusion(nn.Module):\n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.sd_version = sd_version\n",
    "\n",
    "        print(f'[INFO] loading stable diffusion...')\n",
    "        if hf_key is not None:\n",
    "            print(f'[INFO] using hugging face custom model key: {hf_key}')\n",
    "            model_key = hf_key\n",
    "        elif self.sd_version == '2.1':\n",
    "            model_key = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "        elif self.sd_version == '2.0':\n",
    "            model_key = \"stabilityai/stable-diffusion-2-base\"\n",
    "        elif self.sd_version == '1.5':\n",
    "            model_key = \"runwayml/stable-diffusion-v1-5\"\n",
    "        else:\n",
    "            raise ValueError(f'Stable-diffusion version {self.sd_version} not supported.')\n",
    "\n",
    "        # Create model\n",
    "        self.vae = AutoencoderKL.from_pretrained(model_key, subfolder=\"vae\").to(self.device)\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(model_key, subfolder=\"tokenizer\")\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(model_key, subfolder=\"text_encoder\").to(self.device)\n",
    "        self.unet = UNet2DConditionModel.from_pretrained(model_key, subfolder=\"unet\").to(self.device)\n",
    "\n",
    "        self.scheduler = DDIMScheduler.from_pretrained(model_key, subfolder=\"scheduler\")\n",
    "\n",
    "        print(f'[INFO] loaded stable diffusion!')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_text_embeds(self, prompt, negative_prompt):\n",
    "        # prompt, negative_prompt: [str]\n",
    "\n",
    "        # Tokenize text and get embeddings\n",
    "        text_input = self.tokenizer(prompt, padding='max_length', max_length=self.tokenizer.model_max_length,\n",
    "                                    truncation=True, return_tensors='pt')\n",
    "        text_embeddings = self.text_encoder(text_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        # Do the same for unconditional embeddings\n",
    "        uncond_input = self.tokenizer(negative_prompt, padding='max_length', max_length=self.tokenizer.model_max_length,\n",
    "                                      return_tensors='pt')\n",
    "\n",
    "        uncond_embeddings = self.text_encoder(uncond_input.input_ids.to(self.device))[0]\n",
    "\n",
    "        # Cat for final embeddings\n",
    "        text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "        return text_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode_latents(self, latents):\n",
    "        latents = 1 / 0.18215 * latents\n",
    "        imgs = self.vae.decode(latents).sample\n",
    "        imgs = (imgs / 2 + 0.5).clamp(0, 1)\n",
    "        return imgs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def text2panorama(self, prompts, negative_prompts='', height=512, width=2048, num_inference_steps=50,\n",
    "                      guidance_scale=7.5, visualize_intermidiates=False):\n",
    "\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "\n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define panorama grid and get views\n",
    "        latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "        views = get_views(height, width)\n",
    "        count = torch.zeros_like(latent)\n",
    "        value = torch.zeros_like(latent)\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.autocast('cuda'):\n",
    "\n",
    "            if visualize_intermidiates is True:\n",
    "                intermidiate_imgs = []\n",
    "                \n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "                count.zero_()\n",
    "                value.zero_()\n",
    "\n",
    "                for h_start, h_end, w_start, w_end in views:\n",
    "                    # TODO we can support batches, and pass multiple views at once to the unet\n",
    "                    latent_view = latent[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    latent_model_input = torch.cat([latent_view] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # compute the denoising step with the reference model\n",
    "                    latents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)['prev_sample']\n",
    "                    value[:, :, h_start:h_end, w_start:w_end] += latents_view_denoised\n",
    "                    count[:, :, h_start:h_end, w_start:w_end] += 1\n",
    "\n",
    "                # take the MultiDiffusion step\n",
    "                latent = torch.where(count > 0, value / count, value)\n",
    "\n",
    "                # visualize intermidiate timesteps\n",
    "                if visualize_intermidiates is True:\n",
    "                    imgs = self.decode_latents(latent)  # [1, 3, 512, 512]\n",
    "                    img = T.ToPILImage()(imgs[0].cpu())\n",
    "                    intermidiate_imgs.append((i, img))\n",
    "\n",
    "        # Img latents -> imgs\n",
    "        imgs = self.decode_latents(latent)  # [1, 3, 512, 512]\n",
    "        img = T.ToPILImage()(imgs[0].cpu())\n",
    "\n",
    "        if visualize_intermidiates is True:\n",
    "            intermidiate_imgs.append((len(intermidiate_imgs), img))\n",
    "            return intermidiate_imgs\n",
    "        else:\n",
    "            return [img]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How I Warped Your Noise (ERP-Pers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridy2x_pers2erp(gridy, HWy, HWx, THETA, PHI, FOVy, FOVx):\n",
    "    H, W, h, w = *HWy, *HWx\n",
    "    hFOVy, wFOVy = FOVy * float(H) / W, FOVy\n",
    "    hFOVx, wFOVx = FOVx * float(h) / w, FOVx\n",
    "    \n",
    "    # gridy2x\n",
    "    ### onto sphere\n",
    "    gridy = gridy.reshape(-1, 2).float()\n",
    "    gridy[:, 0] *= np.tan(np.radians(hFOVy / 2.0))\n",
    "    gridy[:, 1] *= np.tan(np.radians(wFOVy / 2.0))\n",
    "    gridy = gridy.double().flip(-1)\n",
    "    \n",
    "    x0 = torch.ones(gridy.shape[0], 1)\n",
    "    gridy = torch.cat((x0, gridy), dim=-1)\n",
    "    gridy /= torch.norm(gridy, p=2, dim=-1, keepdim=True)\n",
    "    \n",
    "    ### rotation\n",
    "    y_axis = np.array([0.0, 1.0, 0.0], np.float64)\n",
    "    z_axis = np.array([0.0, 0.0, 1.0], np.float64)\n",
    "    [R1, _] = cv2.Rodrigues(z_axis * np.radians(THETA))\n",
    "    [R2, _] = cv2.Rodrigues(np.dot(R1, y_axis) * np.radians(PHI))   \n",
    "    \n",
    "    gridy = torch.mm(torch.from_numpy(R1), gridy.permute(1, 0)).permute(1, 0)\n",
    "    gridy = torch.mm(torch.from_numpy(R2), gridy.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "    ### sphere to gridx\n",
    "    lat = torch.arcsin(gridy[:, 2]) / np.pi * 2\n",
    "    lon = torch.atan2(gridy[:, 1] , gridy[:, 0]) / np.pi\n",
    "    gridx = torch.stack((lat, lon), dim=-1)\n",
    "\n",
    "    # masky\n",
    "    mask = torch.where(torch.abs(gridx) > 1, 0, 1)\n",
    "    mask = mask[:, 0] * mask[:, 1]\n",
    "\n",
    "    return gridx.float(), mask.float()\n",
    "\n",
    "def gridy2x_erp2pers(gridy, HWy, HWx, THETA, PHI, FOVy, FOVx):\n",
    "    H, W, h, w = *HWy, *HWx\n",
    "    hFOVy, wFOVy = FOVy * float(H) / W, FOVy\n",
    "    hFOVx, wFOVx = FOVx * float(h) / w, FOVx\n",
    "\n",
    "    # gridy2x\n",
    "    ### onto sphere\n",
    "    gridy = gridy.reshape(-1, 2).float()\n",
    "    lat = gridy[:, 0] * np.pi / 2\n",
    "    lon = gridy[:, 1] * np.pi\n",
    "\n",
    "    z0 = torch.sin(lat)\n",
    "    y0 = torch.cos(lat) * torch.sin(lon)\n",
    "    x0 = torch.cos(lat) * torch.cos(lon)\n",
    "    gridy = torch.stack((x0, y0, z0), dim=-1).double()\n",
    "\n",
    "    ### rotation\n",
    "    y_axis = np.array([0.0, 1.0, 0.0], np.float64)\n",
    "    z_axis = np.array([0.0, 0.0, 1.0], np.float64)\n",
    "    [R1, _] = cv2.Rodrigues(z_axis * np.radians(THETA))\n",
    "    [R2, _] = cv2.Rodrigues(np.dot(R1, y_axis) * np.radians(PHI))\n",
    "\n",
    "    R1_inv = torch.inverse(torch.from_numpy(R1))\n",
    "    R2_inv = torch.inverse(torch.from_numpy(R2))\n",
    "\n",
    "    gridy = torch.mm(R2_inv, gridy.permute(1, 0)).permute(1, 0)\n",
    "    gridy = torch.mm(R1_inv, gridy.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "    ### sphere to gridx\n",
    "    z0 = gridy[:, 2] / gridy[:, 0]\n",
    "    y0 = gridy[:, 1] / gridy[:, 0]\n",
    "    gridx = torch.stack((z0, y0), dim=-1).float()\n",
    "\n",
    "    # masky\n",
    "    mask = torch.where(torch.abs(gridx) > 1, 0, 1)\n",
    "    mask = mask[:, 0] * mask[:, 1]\n",
    "    mask *= torch.where(gridy[:, 0] < 0, 0, 1)\n",
    "\n",
    "    return gridx.float(), mask.float()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Naive Implementation\n",
    "- Just sample init. pers noises\n",
    "- and denoise independently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_noise_sampling(src_noise, level=3):\n",
    "    B, C, H, W = src_noise.shape\n",
    "    up_factor = 2 ** level\n",
    "    upscaled_means = F.interpolate(src_noise, scale_factor=(up_factor, up_factor), mode='nearest')\n",
    "    up_H = up_factor * H\n",
    "    up_W = up_factor * W\n",
    "    raw_rand = torch.randn(B, C, up_H, up_W)\n",
    "    Z_mean = raw_rand.unfold(2, up_factor, up_factor).unfold(3, up_factor, up_factor).mean((4, 5))\n",
    "    Z_mean = F.interpolate(Z_mean, scale_factor=up_factor, mode='nearest')\n",
    "    mean_removed_rand = raw_rand - Z_mean\n",
    "    up_noise = upscaled_means / up_factor + mean_removed_rand\n",
    "    return up_noise\n",
    "\n",
    "\n",
    "def get_pers_view_noises(src_noise, src_cfg, tgt_cfg):\n",
    "    up_level = src_cfg[\"up_level\"]\n",
    "    B, C, H_src, W_src = src_noise.shape\n",
    "    if up_level > 1:\n",
    "        src_up_noise = cond_noise_sampling(src_noise, level=up_level)\n",
    "    elif up_level == 1:\n",
    "        src_up_noise = src_noise\n",
    "    else:\n",
    "        NotImplementedError\n",
    "    \n",
    "    # Defining the partitioned polygons for target noise map\n",
    "    H_tgt, W_tgt = tgt_cfg[\"size\"]\n",
    "    tr_H_tgt, tr_W_tgt = 2 * H_tgt + 1, 2 * W_tgt + 1\n",
    "    \n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(tr_H_tgt, dtype=torch.int32),\n",
    "        torch.arange(tr_W_tgt, dtype=torch.int32),\n",
    "        indexing=\"ij\")\n",
    "    mesh_idxs = torch.stack((i, j), dim=-1)\n",
    "    reshaped_mesh_idxs = mesh_idxs.reshape(-1,2)\n",
    "    \n",
    "    front_tri_verts = torch.tensor([\n",
    "        [0, 1, 1+tr_W_tgt], [0, tr_W_tgt, 1+tr_W_tgt], \n",
    "        [tr_W_tgt, 1+tr_W_tgt, 1+2*tr_W_tgt], [tr_W_tgt, 2*tr_W_tgt, 1+2*tr_W_tgt]])\n",
    "    per_tri_verts = torch.cat((front_tri_verts, front_tri_verts + 1),dim=0)\n",
    "    width = torch.arange(0, tr_W_tgt - 1, 2)\n",
    "    height = torch.arange(0, tr_H_tgt-1, 2) * (tr_W_tgt)\n",
    "    start_idxs = (width[None,...] + height[...,None]).reshape(-1,1)\n",
    "    vertices = (start_idxs.repeat(1,8)[...,None] + per_tri_verts[None,...]).reshape(-1,3)\n",
    "    \n",
    "    # Perspective view vertex grid\n",
    "    pers_i, pers_j = torch.meshgrid(\n",
    "        torch.linspace(-1, 1, tr_H_tgt),\n",
    "        torch.linspace(-1, 1, tr_W_tgt),\n",
    "        indexing=\"ij\")\n",
    "    pers_grid = torch.stack((pers_i, pers_j), dim=-1)\n",
    "\n",
    "    res = []\n",
    "    for theta, phi in tgt_cfg[\"view_dirs\"]:\n",
    "        # Warping Rasterized Pers. grid\n",
    "        pers2erp_grid, _ = gridy2x_pers2erp(gridy=pers_grid,\n",
    "            HWy=(2*H_tgt, 2*W_tgt), HWx=(2*H_src, 2*W_src),\n",
    "            THETA=theta, PHI=phi, FOVy=90, FOVx=360)\n",
    "        \n",
    "        tgt_to_src_map = pers2erp_grid.view(tr_H_tgt, tr_W_tgt, 2)\n",
    "        idx_y = reshaped_mesh_idxs[..., 0].int()\n",
    "        idx_x = reshaped_mesh_idxs[..., 1].int()\n",
    "        warped_coords = tgt_to_src_map[idx_y, idx_x].fliplr()\n",
    "\n",
    "        len_grid = idx_y.shape[0]\n",
    "        warped_vtx_pos = torch.cat((warped_coords, torch.zeros(len_grid, 1), torch.ones(len_grid, 1)), dim=-1)\n",
    "        warped_vtx_pos = warped_vtx_pos[None,...].to(\"cuda\")\n",
    "        vertices = vertices.int().to(\"cuda\")\n",
    "\n",
    "        resolution = [H_src * (2 ** up_level), W_src * (2 ** up_level)]\n",
    "        with torch.no_grad():\n",
    "            rast_out, _ = dr.rasterize(glctx, warped_vtx_pos, vertices, resolution=resolution)\n",
    "        rast = rast_out[:,:,:,3:].permute(0,3,1,2).to(torch.int64)\n",
    "\n",
    "        # Finding pixel indices in cond-upsampled map\n",
    "        indices = (rast - 1) // 8 + 1 # there is 8 triangles per pixel\n",
    "        src_up_noise_flat = src_up_noise.reshape(B*C, -1).cpu()\n",
    "        ones_flat = torch.ones_like(src_up_noise_flat[:1])\n",
    "        indices_flat = indices.reshape(1, -1).cpu().to(torch.int64)\n",
    "\n",
    "        # Get warped target noise\n",
    "        fin_v_val = torch.zeros(B*C, H_tgt*W_tgt+1).scatter_add_(1, index=indices_flat.repeat(B*C, 1), src=src_up_noise_flat)[..., 1:]\n",
    "        fin_v_num = torch.zeros(1, H_tgt*W_tgt+1).scatter_add_(1, index=indices_flat, src=ones_flat)[..., 1:]\n",
    "        assert fin_v_num.min() != 0, ValueError(f\"{theta},{phi}\")\n",
    "\n",
    "        final_values = fin_v_val / torch.sqrt(fin_v_num)\n",
    "        tgt_warped_noise = final_values.reshape(B, C, H_tgt, W_tgt).float()\n",
    "        tgt_warped_noise = tgt_warped_noise.cuda()\n",
    "        res.append(tgt_warped_noise)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v3(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.src_cfg = {\"up_level\": 3}\n",
    "        self.tgt_cfg = {\n",
    "            # \"view_dirs\": [\n",
    "            #     (0,0), (0,30), (0,-30), (0,60), (0,-60), (0,90), (0,-90), (0,120), (0,-120),\n",
    "            #     (22.5, 0), (22.5, 30), (22.5, -30), (22.5, 60), (22.5, -60), (22.5, 90), (22.5, -90), (22.5, 90), (22.5, 120), (22.5, -120),\n",
    "            #     (45.0, 0), (45.0, 30), (45.0, -30), (45.0, 60), (45.0, -60), (45.0, 90), (45.0, -90), (45.0, 90), (45.0, 120), (45.0, -120),\n",
    "            # ]\n",
    "            \"view_dirs\": [\n",
    "                (0.0, -45.0), (30.0, -45.0), (60.0, -45.0), (90.0, -45.0), (-30.0, -45.0), (-60.0, -45.0), (-90.0, -45.0),\n",
    "                (0.0, -22.5), (30.0, -22.5), (60.0, -22.5), (90.0, -22.5), (-30.0, -22.5), (-60.0, -22.5), (-90.0, -22.5),\n",
    "                (0.0, 0.0), (30.0, 0.0), (60.0, 0.0), (90.0, 0.0), (-30.0, 0.0), (-60.0, 0.0), (-90.0, 0.0),\n",
    "                (0.0, 22.5), (30.0, 22.5), (60.0, 22.5), (90.0, 22.5), (-30.0, 22.5), (-60.0, 22.5), (-90.0, 22.5),\n",
    "                (0.0, 45.0), (30.0, 45.0), (60.0, 45.0), (90.0, 45.0), (-30.0, 45.0), (-60.0, 45.0), (-90.0, 45.0),\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                  prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, width=1024, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5,\n",
    "                 visualize_intermidiates=False,\n",
    "                 save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define ERP source noise\n",
    "        latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if visualize_intermidiates is True:\n",
    "                intermidiate_imgs = []\n",
    "                \n",
    "            self.tgt_cfg[\"size\"] = (64, 64)\n",
    "            pers_latents = get_pers_view_noises(latent.to(\"cpu\"), self.src_cfg, self.tgt_cfg)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\n",
    "                denoised_pers_latents = []\n",
    "\n",
    "                for latent_view in pers_latents:\n",
    "                    \n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    latent_model_input = torch.cat([latent_view] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # compute the denoising step with the reference model\n",
    "                    latents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)['prev_sample']\n",
    "\n",
    "                    denoised_pers_latents.append(latents_view_denoised)\n",
    "                \n",
    "                pers_latents = denoised_pers_latents\n",
    "\n",
    "                # visualize intermidiate timesteps\n",
    "                if visualize_intermidiates is True:\n",
    "                    pers_img_inps = []\n",
    "                    for k, pers_latent in enumerate(pers_latents):\n",
    "                        pers_img = self.decode_latents(pers_latent)\n",
    "                        pers_img_inps.append((self.tgt_cfg['view_dirs'][k], pers_img))\n",
    "                    intermidiate_imgs.append((i+1, pers_img_inps))\n",
    "                \n",
    "                if save_dir is not None:\n",
    "                    # save image\n",
    "                    if os.path.exists(f\"{save_dir}/{i:0>2}\") is False:\n",
    "                        os.mkdir(f\"{save_dir}/{i:0>2}/\")\n",
    "                    for v, im in pers_img_inps:\n",
    "                        theta, phi = v\n",
    "                        im = ToPILImage()(im[0].cpu())\n",
    "                        im.save(f'/{save_dir}/{i:0>2}/pers_{theta}_{phi}.png')\n",
    "        \n",
    "        return intermidiate_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(2024)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# opt variables\n",
    "sd_version = '2.0'\n",
    "negative = ''\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Realistic cityscape of Florence.\"\n",
    "\n",
    "H, W = 1024, 2048\n",
    "sd = ERPMultiDiffusion_v3(device=device, sd_version=sd_version)\n",
    "\n",
    "dir_name = \"hiwyn\"\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/')\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/{prompt.split(\" \")[0]}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/{prompt.split(\" \")[0]}/')\n",
    "\n",
    "dir = f'/content/{dir_name}/{prompt.split(\" \")[0]}'\n",
    "outputs = sd.text2erp(prompt, negative, height=H, width=W, num_inference_steps=steps, visualize_intermidiates=True, save_dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image\n",
    "dir = f'/content/{dir_name}/{prompt.split(\" \")[0]}'\n",
    "for i, vim in tqdm(outputs):\n",
    "    if os.path.exists(f\"{dir}/{i}\") is False:\n",
    "        os.mkdir(f\"{dir}/{i}/\")\n",
    "    for v, im in vim:\n",
    "        theta, phi = v\n",
    "        im = ToPILImage()(im[0].cpu())\n",
    "        im.save(f'/{dir}/{i}/pers_{theta}_{phi}.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Denoising dependently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cond_noise_sampling(src_noise, level=3):\n",
    "    B, C, H, W = src_noise.shape\n",
    "    up_factor = 2 ** level\n",
    "    upscaled_means = F.interpolate(src_noise, scale_factor=(up_factor, up_factor), mode='nearest')\n",
    "    up_H = up_factor * H\n",
    "    up_W = up_factor * W\n",
    "    raw_rand = torch.randn(B, C, up_H, up_W)\n",
    "    Z_mean = raw_rand.unfold(2, up_factor, up_factor).unfold(3, up_factor, up_factor).mean((4, 5))\n",
    "    Z_mean = F.interpolate(Z_mean, scale_factor=up_factor, mode='nearest')\n",
    "    mean_removed_rand = raw_rand - Z_mean\n",
    "    up_noise = upscaled_means / up_factor + mean_removed_rand\n",
    "    return up_noise\n",
    "\n",
    "\n",
    "def get_pers_view_noises(src_noise, up_level, tgt_cfg):\n",
    "    if up_level > 1:\n",
    "        B, C, H_src, W_src = src_noise.shape\n",
    "        src_up_noise = cond_noise_sampling(src_noise, level=up_level)\n",
    "    elif up_level == 1:\n",
    "        B, C, H_src, W_src = src_noise.shape\n",
    "        H_src, W_src = H_src//8, W_src//8 # hard coding\n",
    "        src_up_noise = src_noise\n",
    "    else:\n",
    "        NotImplementedError\n",
    "    \n",
    "    # Defining the partitioned polygons for target noise map\n",
    "    H_tgt, W_tgt = tgt_cfg[\"size\"]\n",
    "    tr_H_tgt, tr_W_tgt = 2 * H_tgt + 1, 2 * W_tgt + 1\n",
    "    \n",
    "    i, j = torch.meshgrid(\n",
    "        torch.arange(tr_H_tgt, dtype=torch.int32),\n",
    "        torch.arange(tr_W_tgt, dtype=torch.int32),\n",
    "        indexing=\"ij\")\n",
    "    mesh_idxs = torch.stack((i, j), dim=-1)\n",
    "    reshaped_mesh_idxs = mesh_idxs.reshape(-1,2)\n",
    "    \n",
    "    front_tri_verts = torch.tensor([\n",
    "        [0, 1, 1+tr_W_tgt], [0, tr_W_tgt, 1+tr_W_tgt], \n",
    "        [tr_W_tgt, 1+tr_W_tgt, 1+2*tr_W_tgt], [tr_W_tgt, 2*tr_W_tgt, 1+2*tr_W_tgt]])\n",
    "    per_tri_verts = torch.cat((front_tri_verts, front_tri_verts + 1),dim=0)\n",
    "    width = torch.arange(0, tr_W_tgt - 1, 2)\n",
    "    height = torch.arange(0, tr_H_tgt-1, 2) * (tr_W_tgt)\n",
    "    start_idxs = (width[None,...] + height[...,None]).reshape(-1,1)\n",
    "    vertices = (start_idxs.repeat(1,8)[...,None] + per_tri_verts[None,...]).reshape(-1,3)\n",
    "    \n",
    "    # Perspective view vertex grid\n",
    "    pers_i, pers_j = torch.meshgrid(\n",
    "        torch.linspace(-1, 1, tr_H_tgt),\n",
    "        torch.linspace(-1, 1, tr_W_tgt),\n",
    "        indexing=\"ij\")\n",
    "    pers_grid = torch.stack((pers_i, pers_j), dim=-1)\n",
    "\n",
    "    res = []\n",
    "    inds = []\n",
    "    for theta, phi in tgt_cfg[\"view_dirs\"]:\n",
    "        # Warping Rasterized Pers. grid\n",
    "        pers2erp_grid, _ = gridy2x_pers2erp(gridy=pers_grid,\n",
    "            HWy=(2*H_tgt, 2*W_tgt), HWx=(2*H_src, 2*W_src),\n",
    "            THETA=theta, PHI=phi, FOVy=90, FOVx=360)\n",
    "        \n",
    "        tgt_to_src_map = pers2erp_grid.view(tr_H_tgt, tr_W_tgt, 2)\n",
    "        idx_y = reshaped_mesh_idxs[..., 0].int()\n",
    "        idx_x = reshaped_mesh_idxs[..., 1].int()\n",
    "        warped_coords = tgt_to_src_map[idx_y, idx_x].fliplr()\n",
    "\n",
    "        len_grid = idx_y.shape[0]\n",
    "        warped_vtx_pos = torch.cat((warped_coords, torch.zeros(len_grid, 1), torch.ones(len_grid, 1)), dim=-1)\n",
    "        warped_vtx_pos = warped_vtx_pos[None,...].to(\"cuda\")\n",
    "        vertices = vertices.int().to(\"cuda\")\n",
    "\n",
    "        resolution = [H_src * (2 ** up_level), W_src * (2 ** up_level)]\n",
    "        with torch.no_grad():\n",
    "            rast_out, _ = dr.rasterize(glctx, warped_vtx_pos, vertices, resolution=resolution)\n",
    "        rast = rast_out[:,:,:,3:].permute(0,3,1,2).to(torch.int64)\n",
    "\n",
    "        # Finding pixel indices in cond-upsampled map\n",
    "        indices = (rast - 1) // 8 + 1 # there is 8 triangles per pixel\n",
    "        src_up_noise_flat = src_up_noise.reshape(B*C, -1).cpu()\n",
    "        ones_flat = torch.ones_like(src_up_noise_flat[:1])\n",
    "        indices_flat = indices.reshape(1, -1).cpu().to(torch.int64)\n",
    "\n",
    "        # Get warped target noise\n",
    "        fin_v_val = torch.zeros(B*C, H_tgt*W_tgt+1).scatter_add_(1, index=indices_flat.repeat(B*C, 1), src=src_up_noise_flat)[..., 1:]\n",
    "        fin_v_num = torch.zeros(1, H_tgt*W_tgt+1).scatter_add_(1, index=indices_flat, src=ones_flat)[..., 1:]\n",
    "        assert fin_v_num.min() != 0, ValueError(f\"{theta},{phi}\")\n",
    "\n",
    "        final_values = fin_v_val / torch.sqrt(fin_v_num)\n",
    "        tgt_warped_noise = final_values.reshape(B, C, H_tgt, W_tgt).float()\n",
    "        tgt_warped_noise = tgt_warped_noise.cuda()\n",
    "        res.append(tgt_warped_noise)\n",
    "        inds.append(indices.reshape(*resolution).to(torch.int64))\n",
    "        fin_v_num = fin_v_num.reshape(1, 1, H_tgt, W_tgt).cuda()\n",
    "\n",
    "    return res, inds, src_up_noise, fin_v_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Hard-coding by myself\n",
    "\"\"\"\n",
    "def denoise_erp_up_noise(residual_pers_noises, pers_indices, erp_up_noise):\n",
    "    B, C, H_tgt, W_tgt = residual_pers_noises[0].shape\n",
    "    H_up_src, W_up_src = erp_up_noise.shape[-2:]\n",
    "\n",
    "    residual_erp_noise = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "    residual_erp_counts = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "\n",
    "    for residual_pers_noise, indices in zip(residual_pers_noises, pers_indices):\n",
    "        residual_pers_noise_flat = residual_pers_noise.reshape(B, C, -1) # (B, C, H_tgt*W_tgt)\n",
    "        # residual_pers_noise_flat: (B, C, H_tgt*W_tgt) - containing residual noises on perspective grid\n",
    "        # indices: (H_up_src, W_up_src) - containing index on perspective grid to map each ERP pixel to Perspective grid - range: [0, H_tgt*W_tgt], 0 means no-mapping\n",
    "\n",
    "        residual_erp_noise = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "        residual_erp_count = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "\n",
    "        indices = indices - 1\n",
    "        for b in range(0, B):\n",
    "            for c in range(0, C):\n",
    "                for i in range(0, H_up_src):\n",
    "                    for j in range(0, W_up_src):\n",
    "                        idx = indices[i, j]\n",
    "                        if idx == -1:\n",
    "                            pass\n",
    "                        residual_erp_noise[b, c, i, j] += residual_pers_noise_flat[b, c, idx]\n",
    "                        residual_erp_count[b, c, i, j] += 1\n",
    "        \n",
    "    residual_erp_noise = residual_erp_noise / residual_erp_counts\n",
    "    erp_up_noise_denoised = erp_up_noise - residual_erp_noise\n",
    "    return erp_up_noise_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"By GPT\n",
    "\"\"\"\n",
    "def denoise_erp_up_noise(residual_pers_noises, pers_indices, erp_up_noise):\n",
    "    B, C, H_tgt, W_tgt = residual_pers_noises[0].shape\n",
    "    H_up_src, W_up_src = erp_up_noise.shape[-2:]\n",
    "\n",
    "    # 결과를 저장할 텐서 초기화\n",
    "    residual_erp_noise = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "    residual_erp_counts = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "\n",
    "    # `indices` 값이 1부터 시작하므로 1을 빼서 0 기반 인덱스로 변환\n",
    "    pers_indices = [indices - 1 for indices in pers_indices]\n",
    "\n",
    "    # 모든 데이터 처리\n",
    "    for residual_pers_noise, indices in zip(residual_pers_noises, pers_indices):\n",
    "        residual_pers_noise_flat = residual_pers_noise.reshape(B, C, -1)  # (B, C, H_tgt*W_tgt)\n",
    "\n",
    "        # 유효한 인덱스만 필터링\n",
    "        valid_mask = indices >= 0  # 유효한 매핑 위치\n",
    "        valid_indices = indices[valid_mask].view(-1).cuda()  # Flattened valid indices\n",
    "\n",
    "        # 각 ERP 픽셀에 대응하는 residual 값과 카운트를 더하기\n",
    "        for b in range(B):\n",
    "            for c in range(C):\n",
    "                residual_erp_noise[b, c][valid_mask] += residual_pers_noise_flat[b, c, valid_indices]\n",
    "                residual_erp_counts[b, c][valid_mask] += 1\n",
    "\n",
    "    # 평균을 계산하여 denoise 적용  \n",
    "    residual_erp_counts = torch.clamp(residual_erp_counts, min=1)\n",
    "    residual_erp_noise = residual_erp_noise / residual_erp_counts\n",
    "    erp_up_noise_denoised = erp_up_noise - residual_erp_noise\n",
    "    return erp_up_noise_denoised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denoise_erp_up_noise(residual_pers_noises, pers_indices, erp_up_noise):\n",
    "#     B, C, H_tgt, W_tgt = residual_pers_noises[0].shape\n",
    "#     H_up_src, W_up_src = erp_up_noise.shape[-2:]\n",
    "\n",
    "#     # Ensure erp_up_noise is on the correct device\n",
    "#     device = erp_up_noise.device\n",
    "\n",
    "#     # Initialize result tensors\n",
    "#     residual_erp_noise = torch.zeros(B, C, H_up_src, W_up_src, device=device)\n",
    "#     residual_erp_counts = torch.zeros(B, C, H_up_src, W_up_src, device=device)\n",
    "\n",
    "#     # Loop over residual_pers_noises and indices\n",
    "#     for residual_pers_noise, indices in zip(residual_pers_noises, pers_indices):\n",
    "#         residual_pers_noise_flat = residual_pers_noise.reshape(B, C, -1)  # (B, C, H_tgt * W_tgt)\n",
    "\n",
    "#         # Adjust indices (-1 for no mapping)\n",
    "#         indices = indices.to(device) - 1  # Move indices to the correct device and adjust\n",
    "\n",
    "#         # Create a mask to filter out invalid indices (-1)\n",
    "#         valid_mask = indices >= 0\n",
    "\n",
    "#         # Ensure indices are valid for `scatter_add_` and `gather`\n",
    "#         indices_valid = torch.where(valid_mask, indices, torch.zeros_like(indices))\n",
    "\n",
    "#         # Expand indices and mask for batch and channel dimensions\n",
    "#         indices_expanded = indices_valid.unsqueeze(0).unsqueeze(0).expand(B, C, H_up_src, W_up_src)\n",
    "#         valid_mask_expanded = valid_mask.unsqueeze(0).unsqueeze(0).expand(B, C, H_up_src, W_up_src)\n",
    "\n",
    "#         # Map ERP noise using valid indices\n",
    "#         gathered_residuals = torch.zeros_like(residual_erp_noise)\n",
    "#         for b in range(B):\n",
    "#             for c in range(C):\n",
    "#                 gathered_residuals[b, c] = residual_pers_noise_flat[b, c, indices_valid.to(torch.int64)]\n",
    "\n",
    "#         # Accumulate noise and counts\n",
    "#         residual_erp_noise += gathered_residuals * valid_mask_expanded\n",
    "#         residual_erp_counts += valid_mask_expanded.float()\n",
    "\n",
    "#     # Avoid division by zero\n",
    "#     residual_erp_counts = torch.where(residual_erp_counts == 0, torch.ones_like(residual_erp_counts), residual_erp_counts)\n",
    "\n",
    "#     # Compute averaged noise and denoise the input\n",
    "#     residual_erp_noise = residual_erp_noise / residual_erp_counts\n",
    "#     erp_up_noise_denoised = erp_up_noise - residual_erp_noise\n",
    "\n",
    "#     return erp_up_noise_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denoise_erp_up_noise(residual_pers_noises, pers_indices, erp_up_noise):\n",
    "#     B, C, H_tgt, W_tgt = residual_pers_noises[0].shape\n",
    "#     H_up_src, W_up_src = erp_up_noise.shape[-2:]\n",
    "\n",
    "#     # Initialize result tensors\n",
    "#     residual_erp_noise = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "#     residual_erp_counts = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "\n",
    "#     # Loop over residual_pers_noises and indices\n",
    "#     for residual_pers_noise, indices in zip(residual_pers_noises, pers_indices):\n",
    "#         residual_pers_noise_flat = residual_pers_noise.reshape(B, C, -1)  # (B, C, H_tgt * W_tgt)\n",
    "\n",
    "#         # Adjust indices (-1 for no mapping)\n",
    "#         indices = indices - 1  # indices now range from -1 to (H_tgt * W_tgt - 1)\n",
    "\n",
    "#         # Create a mask to filter out invalid indices (-1)\n",
    "#         valid_mask = indices >= 0\n",
    "\n",
    "#         # Ensure indices are valid for `scatter_add_` and `gather`\n",
    "#         indices_valid = torch.where(valid_mask, indices, torch.zeros_like(indices))\n",
    "\n",
    "#         # Expand indices and mask for batch and channel dimensions\n",
    "#         indices_expanded = indices_valid.unsqueeze(0).unsqueeze(0).expand(B, C, H_up_src, W_up_src)\n",
    "#         valid_mask_expanded = valid_mask.unsqueeze(0).unsqueeze(0).expand(B, C, H_up_src, W_up_src)\n",
    "\n",
    "#         # Map ERP noise using valid indices\n",
    "#         gathered_residuals = torch.zeros_like(residual_erp_noise, device=erp_up_noise.device)\n",
    "#         for b in range(B):\n",
    "#             for c in range(C):\n",
    "#                 gathered_residuals[b, c] = residual_pers_noise_flat[b, c, indices_valid]\n",
    "\n",
    "#         # Accumulate noise and counts\n",
    "#         residual_erp_noise += gathered_residuals * valid_mask_expanded\n",
    "#         residual_erp_counts += valid_mask_expanded.float()\n",
    "\n",
    "#     # Avoid division by zero\n",
    "#     residual_erp_counts = torch.where(residual_erp_counts == 0, torch.ones_like(residual_erp_counts), residual_erp_counts)\n",
    "\n",
    "#     # Compute averaged noise and denoise the input\n",
    "#     residual_erp_noise = residual_erp_noise / residual_erp_counts\n",
    "#     erp_up_noise_denoised = erp_up_noise - residual_erp_noise\n",
    "\n",
    "#     return erp_up_noise_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_erp_up_noise(residual_pers_noises, pers_indices, erp_up_noise):\n",
    "    B, C, H_up_src, W_up_src = erp_up_noise.shape\n",
    "    device = erp_up_noise.device\n",
    "\n",
    "    # Initialize tensors to store accumulated noise and count\n",
    "    residual_erp_noise = torch.zeros(B, C, H_up_src, W_up_src, device=device)\n",
    "    residual_erp_counts = torch.zeros(B, C, H_up_src, W_up_src, device=device)\n",
    "\n",
    "    # Preprocess indices to handle zero-indexing and no-mapping\n",
    "    for residual_pers_noise, indices in zip(residual_pers_noises, pers_indices):\n",
    "        # Reshape residual noise to flat representation\n",
    "        residual_pers_noise_flat = residual_pers_noise.reshape(B, C, -1)\n",
    "        \n",
    "        # Adjust indices: subtract 1 to handle zero-indexing, use mask for valid indices\n",
    "        indices_adj = indices - 1\n",
    "        valid_mask = indices_adj != -1\n",
    "\n",
    "        # Use advanced indexing for efficient computation\n",
    "        for b in range(B):\n",
    "            for c in range(C):\n",
    "                # Extract valid indices for this batch and channel\n",
    "                curr_indices = indices_adj[valid_mask[b]]\n",
    "                curr_noise = residual_pers_noise_flat[b, c, curr_indices]\n",
    "                \n",
    "                # Increment noise and count using advanced indexing\n",
    "                residual_erp_noise[b, c][valid_mask[b]] += curr_noise\n",
    "                residual_erp_counts[b, c][valid_mask[b]] += 1\n",
    "\n",
    "    # Avoid division by zero\n",
    "    residual_erp_counts = torch.clamp(residual_erp_counts, min=1)\n",
    "    residual_erp_noise /= residual_erp_counts\n",
    "\n",
    "    # Compute denoised noise\n",
    "    erp_up_noise_denoised = erp_up_noise - residual_erp_noise\n",
    "    return erp_up_noise_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def denoise_erp_up_noise(residual_pers_noises, pers_indices, erp_up_noise):\n",
    "#     B, C, H_tgt, W_tgt = residual_pers_noises[0].shape\n",
    "#     H_up_src, W_up_src = erp_up_noise.shape[-2:]\n",
    "\n",
    "#     # Initialize result tensors\n",
    "#     residual_erp_noise = torch.zeros_like(erp_up_noise, device=erp_up_noise.device)\n",
    "#     residual_erp_counts = torch.zeros_like(erp_up_noise, device=erp_up_noise.device)\n",
    "\n",
    "#     # Loop over residual_pers_noises and indices\n",
    "#     for residual_pers_noise, indices in zip(residual_pers_noises, pers_indices):\n",
    "#         # Flatten residual noise and indices\n",
    "#         residual_pers_noise_flat = residual_pers_noise.reshape(B, C, -1)  # (B, C, H_tgt * W_tgt)\n",
    "#         indices = indices.view(-1)  # Flatten indices to (H_tgt * W_tgt)\n",
    "\n",
    "#         # Create a mask for valid indices\n",
    "#         valid_mask = (indices >= 0)\n",
    "\n",
    "#         # Filter valid indices and corresponding residuals\n",
    "#         valid_indices = indices[valid_mask]  # Only valid indices\n",
    "#         valid_residuals = residual_pers_noise_flat[:, :, valid_indices]  # (B, C, valid_count)\n",
    "\n",
    "#         # Scatter add residuals to the ERP noise grid\n",
    "#         residual_erp_noise.scatter_add_(-1, valid_indices.expand_as(valid_residuals), valid_residuals)\n",
    "\n",
    "#         # Scatter add counts\n",
    "#         residual_erp_counts.scatter_add_(-1, valid_indices.expand_as(valid_residuals), torch.ones_like(valid_residuals))\n",
    "\n",
    "#     # Avoid division by zero\n",
    "#     residual_erp_counts = torch.where(residual_erp_counts == 0, torch.ones_like(residual_erp_counts), residual_erp_counts)\n",
    "\n",
    "#     # Compute averaged noise and denoise the input\n",
    "#     residual_erp_noise = residual_erp_noise / residual_erp_counts\n",
    "#     erp_up_noise_denoised = erp_up_noise - residual_erp_noise\n",
    "\n",
    "#     return erp_up_noise_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_erp_up_noise(residual_pers_noises, pers_indices, erp_up_noise):\n",
    "    B, C, H_tgt, W_tgt = residual_pers_noises[0].shape\n",
    "    H_up_src, W_up_src = erp_up_noise.shape[-2:]\n",
    "\n",
    "    # Initialize result tensors\n",
    "    residual_erp_noise = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "    residual_erp_counts = torch.zeros(B, C, H_up_src, W_up_src, device=erp_up_noise.device)\n",
    "\n",
    "    # Loop over residual_pers_noises and indices\n",
    "    for residual_pers_noise, indices in zip(residual_pers_noises, pers_indices):\n",
    "        residual_pers_noise_flat = residual_pers_noise.reshape(B, C, -1)  # (B, C, H_tgt * W_tgt)\n",
    "\n",
    "        # Adjust indices (-1 for no mapping)\n",
    "        indices = indices - 1  # indices now range from -1 to (H_tgt * W_tgt - 1)\n",
    "\n",
    "        # Create a mask to filter out invalid indices (-1)\n",
    "        valid_mask = indices >= 0\n",
    "\n",
    "        # Expand indices for batch and channel dimensions\n",
    "        indices_expanded = indices.unsqueeze(0).unsqueeze(0).expand(B, C, H_up_src, W_up_src)\n",
    "        valid_mask_expanded = valid_mask.unsqueeze(0).unsqueeze(0).expand(B, C, H_up_src, W_up_src)\n",
    "\n",
    "        # Use advanced indexing to accumulate residual noise\n",
    "        residual_erp_noise.scatter_add_(\n",
    "            dim=2,\n",
    "            index=indices_expanded * valid_mask_expanded,\n",
    "            src=residual_pers_noise_flat.gather(dim=2, index=indices_expanded),\n",
    "        )\n",
    "\n",
    "        # Count valid mappings\n",
    "        residual_erp_counts.scatter_add_(\n",
    "            dim=2,\n",
    "            index=indices_expanded * valid_mask_expanded,\n",
    "            src=valid_mask_expanded.float()\n",
    "        )\n",
    "\n",
    "    # Avoid division by zero\n",
    "    residual_erp_counts = torch.where(residual_erp_counts == 0, torch.ones_like(residual_erp_counts), residual_erp_counts)\n",
    "\n",
    "    # Compute averaged noise and denoise the input\n",
    "    residual_erp_noise = residual_erp_noise / residual_erp_counts\n",
    "    erp_up_noise_denoised = erp_up_noise - residual_erp_noise\n",
    "\n",
    "    return erp_up_noise_denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v3_2(MultiDiffusion):\n",
    "    \n",
    "    def __init__(self, device, sd_version='2.0', hf_key=None):\n",
    "        super().__init__(device, sd_version, hf_key)\n",
    "        self.up_level = 3\n",
    "        self.tgt_cfg = {\n",
    "            # \"view_dirs\": [\n",
    "            #     (0,0), (0,30), (0,-30), (0,60), (0,-60), (0,90), (0,-90), (0,120), (0,-120),\n",
    "            #     (22.5, 0), (22.5, 30), (22.5, -30), (22.5, 60), (22.5, -60), (22.5, 90), (22.5, -90), (22.5, 90), (22.5, 120), (22.5, -120),\n",
    "            #     (45.0, 0), (45.0, 30), (45.0, -30), (45.0, 60), (45.0, -60), (45.0, 90), (45.0, -90), (45.0, 90), (45.0, 120), (45.0, -120),\n",
    "            # ]\n",
    "            # \"view_dirs\": [\n",
    "            #     (0.0, -45.0), (30.0, -45.0), (60.0, -45.0), (90.0, -45.0), (-30.0, -45.0), (-60.0, -45.0), (-90.0, -45.0),\n",
    "            #     (0.0, -22.5), (30.0, -22.5), (60.0, -22.5), (90.0, -22.5), (-30.0, -22.5), (-60.0, -22.5), (-90.0, -22.5),\n",
    "            #     (0.0, 0.0), (30.0, 0.0), (60.0, 0.0), (90.0, 0.0), (-30.0, 0.0), (-60.0, 0.0), (-90.0, 0.0),\n",
    "            #     (0.0, 22.5), (30.0, 22.5), (60.0, 22.5), (90.0, 22.5), (-30.0, 22.5), (-60.0, 22.5), (-90.0, 22.5),\n",
    "            #     (0.0, 45.0), (30.0, 45.0), (60.0, 45.0), (90.0, 45.0), (-30.0, 45.0), (-60.0, 45.0), (-90.0, 45.0),\n",
    "            # ]\n",
    "            # \"view_dirs\": [\n",
    "            #     (0.0, -22.5), (30.0, -22.5), (60.0, -22.5), (-30.0, -22.5), (-60.0, -22.5),\n",
    "            #     (0.0, 0.0), (30.0, 0.0), (60.0, 0.0), (-30.0, 0.0), (-60.0, 0.0),\n",
    "            #     (0.0, 22.5), (30.0, 22.5), (60.0, 22.5), (-30.0, 22.5), (-60.0, 22.5),\n",
    "            # ]\n",
    "            \"view_dirs\": [\n",
    "                (0.0, -22.5), (30.0, -22.5), (-30.0, -22.5),\n",
    "                (0.0, 0.0), (30.0, 0.0), (-30.0, 0.0),\n",
    "                (0.0, 22.5), (30.0, 22.5), (-30.0, 22.5),\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def text2erp(self,\n",
    "                 prompts, \n",
    "                 negative_prompts='', \n",
    "                 height=512, width=1024, \n",
    "                 num_inference_steps=50,\n",
    "                 guidance_scale=7.5,\n",
    "                 visualize_intermidiates=False,\n",
    "                 save_dir=None):\n",
    "        \n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        if isinstance(negative_prompts, str):\n",
    "            negative_prompts = [negative_prompts]\n",
    "\n",
    "        # Prompts -> text embeds\n",
    "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "        # Define ERP source noise\n",
    "        latent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if visualize_intermidiates is True:\n",
    "                intermidiate_imgs = []\n",
    "                \n",
    "            self.tgt_cfg[\"size\"] = (64, 64)\n",
    "            pers_latents, pers_indices, erp_up_noise, fin_v_num =\\\n",
    "                get_pers_view_noises(latent.to(\"cpu\"), self.up_level, self.tgt_cfg)\n",
    "\n",
    "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\n",
    "                denoised_pers_latents = []\n",
    "                residual_pers_noises = []\n",
    "\n",
    "                for latent_view in pers_latents:\n",
    "                    \n",
    "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "                    latent_model_input = torch.cat([latent_view] * 2)\n",
    "\n",
    "                    # predict the noise residual\n",
    "                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "                    # perform guidance\n",
    "                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                    # compute the denoising step with the reference model\n",
    "                    latents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)['prev_sample']\n",
    "                    denoised_pers_latents.append(latents_view_denoised)\n",
    "                    \n",
    "                    # compute residual noise\n",
    "                    residual_noise = latent_view - latents_view_denoised\n",
    "                    residual_noise = residual_noise * torch.sqrt(fin_v_num)\n",
    "                    residual_pers_noises.append(residual_noise)\n",
    "\n",
    "                erp_up_noise_denoised = denoise_erp_up_noise(residual_pers_noises, pers_indices, erp_up_noise)\n",
    "\n",
    "                pers_latents, _, erp_up_noise, _ =\\\n",
    "                    get_pers_view_noises(erp_up_noise_denoised, 1, self.tgt_cfg)\n",
    "\n",
    "                # visualize intermidiate timesteps\n",
    "                if visualize_intermidiates is True:\n",
    "                    pers_img_inps = []\n",
    "                    for k, pers_latent in enumerate(pers_latents):\n",
    "                        pers_img = self.decode_latents(pers_latent)\n",
    "                        pers_img_inps.append((self.tgt_cfg['view_dirs'][k], pers_img))\n",
    "                    intermidiate_imgs.append((i+1, pers_img_inps))\n",
    "                \n",
    "                if save_dir is not None:\n",
    "                    # save image\n",
    "                    if os.path.exists(f\"{save_dir}/{i:0>2}\") is False:\n",
    "                        os.mkdir(f\"{save_dir}/{i:0>2}/\")\n",
    "                    for v, im in pers_img_inps:\n",
    "                        theta, phi = v\n",
    "                        im = ToPILImage()(im[0].cpu())\n",
    "                        im.save(f'/{save_dir}/{i:0>2}/pers_{theta}_{phi}.png')\n",
    "        \n",
    "        return intermidiate_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "seed_everything(2024)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# opt variables\n",
    "sd_version = '2.0'\n",
    "negative = ''\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Realistic cityscape of Florence.\"\n",
    "\n",
    "H, W = 1024, 2048\n",
    "sd = ERPMultiDiffusion_v3_2(device=device, sd_version=sd_version)\n",
    "\n",
    "dir_name = \"hiwyn\"\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/')\n",
    "\n",
    "if os.path.exists(f'/content/{dir_name}/{prompt.split(\" \")[0]}/') is False:\n",
    "    os.mkdir(f'/content/{dir_name}/{prompt.split(\" \")[0]}/')\n",
    "\n",
    "dir = f'/content/{dir_name}/{prompt.split(\" \")[0]}'\n",
    "outputs = sd.text2erp(prompt, negative, height=H, width=W, num_inference_steps=steps, visualize_intermidiates=True, save_dir=dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
