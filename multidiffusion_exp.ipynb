{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install transformers diffusers tensorboardX\n",
        "!git clone https://github.com/ByeongHyunPak/omni-proj.git\n",
        "\n",
        "import os\n",
        "os.chdir('/content/omni-proj/omni_proj')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MultiDiffusion for 360 degree Panorama image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import CLIPTextModel, CLIPTokenizer, logging\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, DDIMScheduler\n",
        "\n",
        "# suppress partial model loading warning\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import utils as utils\n",
        "from utils import gridy2x_erp2pers, gridy2x_pers2erp\n",
        "from multidiffusion import MultiDiffusion, seed_everything\n",
        "\n",
        "class ERPMultiDiffusion(MultiDiffusion):\n",
        "    def __init__(self, num_rows=4, single_pers=False, **kwargs):\n",
        "        super(ERPMultiDiffusion, self).__init__(**kwargs)      \n",
        "        \n",
        "        if num_rows == 3:\n",
        "            num_cols = [3, 5, 3]\n",
        "            phi_centers = [-67.5, 0.0, 67.5]\n",
        "\n",
        "        elif num_rows == 4:\n",
        "            # num_cols = [3, 6, 6, 3]\n",
        "            # phi_centers = [-67.5, -22.5, 22.5, 67.5]\n",
        "\n",
        "            num_cols = [1, 6, 6, 1]\n",
        "            phi_centers = [-90.0, -22.5, 22.5, 90.0]\n",
        "\n",
        "        self.single_pers = single_pers\n",
        "        self.pers_centers = self.get_pers_centers(num_cols, phi_centers)\n",
        "        \n",
        "    def get_pers_centers(self, num_cols, phi_centers):\n",
        "        pers_centers = []\n",
        "        for i, n_cols in enumerate(num_cols):\n",
        "            PHI = phi_centers[i]\n",
        "            for j in np.arange(n_cols):\n",
        "                theta_interval = 360 / n_cols\n",
        "                THETA = j * theta_interval + theta_interval / 2\n",
        "                pers_centers.append((THETA, PHI))\n",
        "        if self.single_pers is True:\n",
        "            rand_idx = random.randint(0, len(pers_centers)-1)\n",
        "            pers_centers = pers_centers[rand_idx]\n",
        "        return pers_centers\n",
        "    \n",
        "    def projection(self, x_inp, projy2x, projx2y, THETA, PHI, FOVy, FOVx, HWy):\n",
        "        device = x_inp.device\n",
        "        HWx = x_inp.shape[-2:]\n",
        "\n",
        "        gridy = utils.make_coord(HWy).to(device)\n",
        "        gridy2x, masky = projy2x(gridy, HWy, HWx, THETA, PHI, FOVy, FOVx, device)\n",
        "        gridy2x, masky = gridy2x.view(*HWy, 2), masky.view(1, *HWy)\n",
        "\n",
        "        y_inp = F.grid_sample(\n",
        "                    x_inp, gridy2x.unsqueeze(0).flip(-1),\n",
        "                    mode='nearest', padding_mode='reflection',\n",
        "                    align_corners=True).clamp_(x_inp.min(), x_inp.max())\n",
        "        y_inp = y_inp * masky\n",
        "\n",
        "        gridx = utils.make_coord(HWx, flatten=False).to(device)\n",
        "        _, maskx = projx2y(gridx, HWy, HWx, THETA, PHI, FOVy, FOVx, device)\n",
        "        maskx = maskx.view(1, *HWx)\n",
        "\n",
        "        return y_inp, masky, maskx\n",
        "\n",
        "    def erp2pers(self, \n",
        "                 erp_inp, \n",
        "                 pers_size=(512//8, 512//8)):\n",
        "        \n",
        "        # # Upscale the ERP input before projection\n",
        "        # erp_inp = F.interpolate(erp_inp,\n",
        "        #     size=(erp_inp.shape[-2]*4, erp_inp.shape[-1]*4), \n",
        "        #     mode='bicubic', align_corners=True)\n",
        "        \n",
        "        # ERP to Perspective\n",
        "        pers_outs = []\n",
        "        for THETA, PHI in self.pers_centers:\n",
        "            pers_out, _, _ = self.projection(\n",
        "                erp_inp, gridy2x_erp2pers, gridy2x_pers2erp, \n",
        "                THETA, PHI, FOVy=90, FOVx=360, HWy=pers_size)\n",
        "            pers_outs.append(pers_out)\n",
        "\n",
        "        return pers_outs\n",
        "    \n",
        "    def pers2erp(self,\n",
        "                 pers_inps,\n",
        "                 erp_size=(1024//8, 2048//8)):\n",
        "        \n",
        "        erp_outs = None\n",
        "        count = None\n",
        "        for i, pers_inp in enumerate(pers_inps):\n",
        "            # # Upscale the Pers input before projection\n",
        "            # pers_inp = F.interpolate(pers_inp,\n",
        "            #     size=(pers_inp.shape[-2]*4, pers_inp.shape[-1]*4),\n",
        "            #     mode='bicubic', align_corners=True)\n",
        "        \n",
        "            # Perspective to ERP\n",
        "            THETA, PHI = self.pers_centers[i]\n",
        "            erp_out, erp_mask, _ = self.projection(\n",
        "                pers_inp, gridy2x_pers2erp, gridy2x_erp2pers,\n",
        "                THETA, PHI, FOVy=360, FOVx=90, HWy=erp_size)\n",
        "            \n",
        "            if erp_outs is None:\n",
        "                erp_outs = erp_out\n",
        "            else:\n",
        "                erp_outs += erp_out\n",
        "            if count is None:\n",
        "                count = erp_mask\n",
        "            else:\n",
        "                count += erp_mask\n",
        "        erp_outs = torch.where(count > 0, erp_outs / count, erp_outs)\n",
        "\n",
        "        return erp_outs\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def text2erp(self, \n",
        "                 prompts, \n",
        "                 negative_prompts='', \n",
        "                 height=1024, width=2048, \n",
        "                 num_inference_steps=50,\n",
        "                 guidance_scale=7.5,\n",
        "                 visualize_intermidiates=False):\n",
        "        \n",
        "        if isinstance(prompts, str):\n",
        "            prompts = [prompts]\n",
        "\n",
        "        if isinstance(negative_prompts, str):\n",
        "            negative_prompts = [negative_prompts]\n",
        "\n",
        "        # Prompts -> text embeds\n",
        "        text_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
        "\n",
        "        # Initialize ERP noise\n",
        "        erp_latent = torch.randn((1, self.unet.in_channels, height//8, width//8), device=self.device)\n",
        "        \n",
        "        self.scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            if visualize_intermidiates is True:\n",
        "                  intermidiate_imgs = []\n",
        "\n",
        "            for i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
        "\n",
        "                denoised_pers_latents = []\n",
        "                \n",
        "                # get latents on pers. grid\n",
        "                pers_latents = self.erp2pers(erp_latent)\n",
        "\n",
        "                for latent_view in pers_latents:\n",
        "\n",
        "                    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "                    latent_model_input = torch.cat([latent_view] * 2)\n",
        "\n",
        "                    # predict the noise residual\n",
        "                    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
        "\n",
        "                    # perform guidance\n",
        "                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
        "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
        "\n",
        "                    # compute the denoising step with the reference model\n",
        "                    latents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)['prev_sample']\n",
        "\n",
        "                    denoised_pers_latents.append(latents_view_denoised)\n",
        "\n",
        "                pers_latents = denoised_pers_latents\n",
        "\n",
        "                erp_latent = self.pers2erp(denoised_pers_latents, erp_size=(height//8, width//8))\n",
        "\n",
        "                # visualize intermidiate timesteps\n",
        "                if visualize_intermidiates is True:\n",
        "                    pers_img_inps = []\n",
        "                    for pers_latent in pers_latents:\n",
        "                        pers_img = self.decode_latents(pers_latent)\n",
        "                        pers_img_inps.append(pers_img)\n",
        "                    erp_img = T.ToPILImage()(self.pers2erp(pers_img_inps, erp_size=(height, width))[0].cpu())\n",
        "                    intermidiate_imgs.append((i, erp_img))\n",
        "\n",
        "        pers_img_inps = []\n",
        "        for pers_latent in pers_latents:\n",
        "            pers_img = self.decode_latents(pers_latent)\n",
        "            pers_img_inps.append(pers_img)\n",
        "        \n",
        "        erp_img = T.ToPILImage()(self.pers2erp(pers_img_inps, erp_size=(height, width))[0].cpu())\n",
        "\n",
        "        if visualize_intermidiates is True:\n",
        "            intermidiate_imgs.append((len(intermidiate_imgs), erp_img))\n",
        "            return intermidiate_imgs, pers_img_inps\n",
        "        else:\n",
        "          return [erp_img], pers_img_inps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed_everything(2024)\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# opt variables\n",
        "sd_version = '2.0'\n",
        "negative = ''\n",
        "steps = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" MultiDiffusion exp.\n",
        "\"\"\"\n",
        "prompt  = \"firenze cityscpae\"\n",
        "H = 512\n",
        "W = 512\n",
        "\n",
        "if os.path.exists(f'/content/md/') is False:\n",
        "    os.mkdir(f'/content/md/')\n",
        "\n",
        "if os.path.exists(f'/content/md/{prompt.split(' ')[0]}/') is False:\n",
        "    os.mkdir(f'/content/md/{prompt.split(' ')[0]}/')\n",
        "\n",
        "sd = MultiDiffusion(device, sd_version)\n",
        "\n",
        "img = sd.text2panorama(prompt, negative, H, W, steps, visualize_intermidiates=True)\n",
        "\n",
        "# save image\n",
        "dir = f'/content/md/{prompt.splat(' ')[0]}'\n",
        "if len(img) == 1:\n",
        "    img[0].save(f'{dir}/output.png')\n",
        "else:\n",
        "    for t, im in tqdm(img):\n",
        "        im.save(f'{dir}/output_t={t:02d}.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" ERP MultiDiffusion exp.\n",
        "\"\"\"\n",
        "prompt  = \"firenze cityscpae\"\n",
        "H = 1024\n",
        "W = 2048\n",
        "\n",
        "single_pers = False\n",
        "\n",
        "if single_pers:\n",
        "   dir_name = \"single_erp_md\"\n",
        "else:\n",
        "   dir_name = \"erp_md\"\n",
        "   \n",
        "if os.path.exists(f'/content/{dir_name}/') is False:\n",
        "    os.mkdir(f'/content/{dir_name}/')\n",
        "\n",
        "if os.path.exists(f'/content/{dir_name}/{prompt.split(' ')[0]}/') is False:\n",
        "    os.mkdir(f'/content/{dir_name}/{prompt.split(' ')[0]}/')\n",
        "\n",
        "sd = ERPMultiDiffusion(num_rows=3, device=device, sd_version=sd_version, single_pers=single_pers)\n",
        "\n",
        "erp_img, pers_imgs = sd.text2erp(prompt, negative, num_inference_steps=steps, visualize_intermidiates=True)\n",
        "\n",
        "# save image\n",
        "dir = f'/content/{dir_name}/{prompt.split(' '[0])}'\n",
        "\n",
        "if len(erp_img) == 1:\n",
        "  erp_img[0].save(f'{dir}/erp_output.png')\n",
        "else:\n",
        "  for t, im in tqdm(erp_img):\n",
        "    im.save(f'/{dir}/erp_output_t={t:02d}.png')\n",
        "\n",
        "if os.path.exists(f'/{dir}/pers/') is False:\n",
        "    os.mkdir(f'/{dir}/pers/')\n",
        "\n",
        "for i, pers_img in enumerate(pers_imgs):\n",
        "  pers_img = T.ToPILImage()(pers_img[0].cpu())\n",
        "  pers_img.save(f'/{dir}/pers/pers_output_i={i:02d}.png')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "name": "scratchpad",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
