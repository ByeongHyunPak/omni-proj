{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers diffusers tensorboardX\n",
    "!git clone https://github.com/ByeongHyunPak/omni-proj.git\n",
    "\n",
    "import os\n",
    "os.chdir('/content/omni-proj/omni_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import utils\n",
    "from utils import gridy2x_erp2pers, gridy2x_pers2erp\n",
    "from multidiffusions import MultiDiffusion, seed_everything, get_views\n",
    "\n",
    "seed_everything(2024)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# opt variables\n",
    "sd_version = '2.0'\n",
    "negative = ''\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v2(MultiDiffusion): \n",
    "\tdef __init__(self, **kwargs):\n",
    "\t\tsuper(ERPMultiDiffusion_v2, self).__init__(**kwargs)\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef text2erp(self, \n",
    "\t\t\t\t prompts, \n",
    "\t\t\t\t negative_prompts='', \n",
    "\t\t\t\t height=512, width=1024, \n",
    "\t\t\t\t num_inference_steps=50,\n",
    "\t\t\t\t guidance_scale=7.5,\n",
    "\t\t\t\t visualize_intermidiates=False):\n",
    "\t\t\n",
    "\t\tif isinstance(prompts, str):\n",
    "\t\t\tprompts = [prompts]\n",
    "\n",
    "\t\tif isinstance(negative_prompts, str):\n",
    "\t\t\tnegative_prompts = [negative_prompts]\n",
    "\n",
    "\t\t# Prompts -> text embeds\n",
    "\t\ttext_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "\t\t# Define panorama grid and get views\n",
    "\t\tlatent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\t\tviews = get_views(height, width)\n",
    "\t\tcount = torch.zeros_like(latent)\n",
    "\t\tvalue = torch.zeros_like(latent)\n",
    "\n",
    "\t\tself.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t\n",
    "\t\t\tif visualize_intermidiates is True:\n",
    "\t\t\t\tintermidiate_imgs = []\n",
    "\t\t\t\n",
    "\t\t\tfor i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\t\t\t\tcount.zero_()\n",
    "\t\t\t\tvalue.zero_()\n",
    "\n",
    "\t\t\t\tfor h_start, h_end, w_start, w_end in views:\n",
    "\t\t\t\t\t# TODO we can support batches, and pass multiple views at once to the unet\n",
    "\t\t\t\t\tlatent_view = latent[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "\t\t\t\t\t# expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "\t\t\t\t\tlatent_model_input = torch.cat([latent_view] * 2)\n",
    "\n",
    "\t\t\t\t\t# predict the noise residual\n",
    "\t\t\t\t\tnoise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "\t\t\t\t\t# perform guidance\n",
    "\t\t\t\t\tnoise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "\t\t\t\t\tnoise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "\t\t\t\t\t# compute the denoising step with the reference model\n",
    "\t\t\t\t\tlatents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)['prev_sample']\n",
    "\t\t\t\t\tvalue[:, :, h_start:h_end, w_start:w_end] += latents_view_denoised\n",
    "\t\t\t\t\tcount[:, :, h_start:h_end, w_start:w_end] += 1\n",
    "\n",
    "\t\t\t# take the MultiDiffusion step\n",
    "\t\t\tlatent = torch.where(count > 0, value / count, value)\n",
    "\n",
    "\t\t\t# visualize intermidiate timesteps\n",
    "\t\t\tif visualize_intermidiates is True:\n",
    "\t\t\t\timgs = self.decode_latents(latent)  # [1, 3, 512, 512]\n",
    "\t\t\t\timg = T.ToPILImage()(imgs[0].cpu())\n",
    "\t\t\t\tintermidiate_imgs.append((i, img))\n",
    "\n",
    "\t\t# Img latents -> imgs\n",
    "\t\timgs = self.decode_latents(latent)  # [1, 3, 512, 512]\n",
    "\t\timg = T.ToPILImage()(imgs[0].cpu())\n",
    "\n",
    "\t\tif visualize_intermidiates is True:\n",
    "\t\t\tintermidiate_imgs.append((len(intermidiate_imgs), img))\n",
    "\t\t\treturn intermidiate_imgs\n",
    "\t\telse:\n",
    "\t\t\treturn [img]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
