{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers diffusers tensorboardX\n",
    "!git clone https://github.com/ByeongHyunPak/omni-proj.git\n",
    "\n",
    "import os\n",
    "os.chdir('/content/omni-proj/omni_proj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import utils\n",
    "from utils import gridy2x_erp2pers, gridy2x_pers2erp\n",
    "from multidiffusions import MultiDiffusion, seed_everything, get_views\n",
    "\n",
    "seed_everything(2024)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# opt variables\n",
    "sd_version = '2.0'\n",
    "negative = ''\n",
    "steps = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERPMultiDiffusion_v2(MultiDiffusion): \n",
    "\tdef __init__(self, latent_rotation, circular_padding, **kwargs):\n",
    "\t\tsuper(ERPMultiDiffusion_v2, self).__init__(**kwargs)\n",
    "\n",
    "\t\tself.latent_rotation = latent_rotation\n",
    "\t\tself.circular_padding = circular_padding\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef decode_latents(self, latents):\n",
    "\t\timgs = super().decode_latents(latents)\n",
    "\t\tif self.circular_padding:\n",
    "\t\t\tw = imgs.shape[-1] // 4\n",
    "\t\t\timgs = imgs[:,:,:, w:-w]\n",
    "\t\treturn imgs\n",
    "\n",
    "\t@torch.no_grad()\n",
    "\tdef text2erp(self, \n",
    "\t\t\t\t prompts, \n",
    "\t\t\t\t negative_prompts='', \n",
    "\t\t\t\t height=512, width=1024, \n",
    "\t\t\t\t num_inference_steps=50,\n",
    "\t\t\t\t guidance_scale=7.5,\n",
    "\t\t\t\t visualize_intermidiates=False):\n",
    "\t\t\n",
    "\t\tif isinstance(prompts, str):\n",
    "\t\t\tprompts = [prompts]\n",
    "\n",
    "\t\tif isinstance(negative_prompts, str):\n",
    "\t\t\tnegative_prompts = [negative_prompts]\n",
    "\n",
    "\t\t# Prompts -> text embeds\n",
    "\t\ttext_embeds = self.get_text_embeds(prompts, negative_prompts)  # [2, 77, 768]\n",
    "\n",
    "\t\t# Define panorama grid and get views\n",
    "\t\tlatent = torch.randn((1, self.unet.in_channels, height // 8, width // 8), device=self.device)\n",
    "\t\tif self.circular_padding:\n",
    "\t\t\tlatent = torch.cat((latent[:,:,:,width // 16:], latent, latent[:,:,:,:width // 16]), dim=-1) # - circular padding\n",
    "\t\t\tviews = get_views(height, 2 * width)\n",
    "\t\telse:\n",
    "\t\t\tviews = get_views(height, width)\n",
    "\t\tcount = torch.zeros_like(latent)\n",
    "\t\tvalue = torch.zeros_like(latent)\n",
    "\n",
    "\t\tself.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t\n",
    "\t\t\tif visualize_intermidiates:\n",
    "\t\t\t\tintermidiate_imgs = []\n",
    "\t\t\t\n",
    "\t\t\tfor i, t in enumerate(tqdm(self.scheduler.timesteps)):\n",
    "\t\t\t\tcount.zero_()\n",
    "\t\t\t\tvalue.zero_()\n",
    "\n",
    "\t\t\t\tfor h_start, h_end, w_start, w_end in views:\n",
    "\t\t\t\t\t# TODO we can support batches, and pass multiple views at once to the unet\n",
    "\t\t\t\t\tlatent_view = latent[:, :, h_start:h_end, w_start:w_end]\n",
    "\n",
    "\t\t\t\t\t# expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "\t\t\t\t\tlatent_model_input = torch.cat([latent_view] * 2)\n",
    "\n",
    "\t\t\t\t\t# predict the noise residual\n",
    "\t\t\t\t\tnoise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeds)['sample']\n",
    "\n",
    "\t\t\t\t\t# perform guidance\n",
    "\t\t\t\t\tnoise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "\t\t\t\t\tnoise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "\t\t\t\t\t# compute the denoising step with the reference model\n",
    "\t\t\t\t\tlatents_view_denoised = self.scheduler.step(noise_pred, t, latent_view)['prev_sample']\n",
    "\t\t\t\t\tvalue[:, :, h_start:h_end, w_start:w_end] += latents_view_denoised\n",
    "\t\t\t\t\tcount[:, :, h_start:h_end, w_start:w_end] += 1\n",
    "\n",
    "\t\t\t\t# take the MultiDiffusion step\n",
    "\t\t\t\tlatent = torch.where(count > 0, value / count, value)\n",
    "\n",
    "\t\t\t\tif self.circular_padding:\n",
    "\t\t\t\t\tlatent = latent[:,:,:,width//16:-width//16] # - circular unpadding\n",
    "\t\t\t\tif self.latent_rotation == \"horizontal_only\":\n",
    "\t\t\t\t\tw = width//8\n",
    "\t\t\t\t\tlatent = torch.roll(latent, int(w / num_inference_steps), dims=-1) # - latent rotation\n",
    "\t\t\t\telif self.latent_rotation == \"vertical_too\":\n",
    "\t\t\t\t\th, w = height//8, width//8\n",
    "\t\t\t\t\tlatent = torch.roll(latent, (int(h / num_inference_steps), int(w / num_inference_steps)), dims=(-2,-1)) # - latent rotation\n",
    "\t\t\t\tif self.circular_padding:\n",
    "\t\t\t\t\tlatent = torch.cat((latent[:,:,:,width // 16:], latent, latent[:,:,:,:width // 16]), dim=-1) # - circular padding\n",
    "\n",
    "\t\t\t\t# visualize intermidiate timesteps\n",
    "\t\t\t\tif visualize_intermidiates:\n",
    "\t\t\t\t\timgs = self.decode_latents(latent)  # [1, 3, 512, 1024]\n",
    "\t\t\t\t\timg = T.ToPILImage()(imgs[0].cpu())\n",
    "\t\t\t\t\tintermidiate_imgs.append((i, img))\n",
    "\n",
    "\t\t# Img latents -> imgs\n",
    "\t\timgs = self.decode_latents(latent)  # [1, 3, 512, 1024]\n",
    "\t\timg = T.ToPILImage()(imgs[0].cpu())\n",
    "\n",
    "\t\tif visualize_intermidiates:\n",
    "\t\t\tintermidiate_imgs.append((len(intermidiate_imgs), img))\n",
    "\t\t\treturn intermidiate_imgs\n",
    "\t\telse:\n",
    "\t\t\treturn [img]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ERPMultiDiffusion_v2 Exp.\n",
    "\"\"\"\n",
    "prompt  = \"360-degree panoramic image, Japanese anime style downtown city street\"\n",
    "H = 512\n",
    "W = 1024\n",
    "\n",
    "dir = f'/content/emd2/{prompt.split(\" \")[3]}'\n",
    "\n",
    "if os.path.exists(f'/content/emd2/') is False:\n",
    "    os.mkdir(f'/content/emd2/')\n",
    "\n",
    "if os.path.exists(dir) is False:\n",
    "    os.mkdir(dir)\n",
    "\n",
    "\n",
    "latent_rotation = \"vertical_too\" # horizontal_only / vertical_too / none\n",
    "circular_padding = True\n",
    "\n",
    "sd = ERPMultiDiffusion_v2(latent_rotation, circular_padding, device=device, sd_version=sd_version)\n",
    "\n",
    "img = sd.text2erp(prompt, negative, H, W, steps, visualize_intermidiates=True)\n",
    "\n",
    "# save image\n",
    "\n",
    "if len(img) == 1:\n",
    "    img[0].save(f'{dir}/output.png')\n",
    "else:\n",
    "    for t, im in tqdm(img):\n",
    "        im.save(f'{dir}/output_t={t:02d}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a - latent_rotation=True; circular_padding=True\n",
    "b - latent_rotation=False; circular_padding=True\n",
    "c - latent_rotation=True; circular_padding=False\n",
    "d - latent_rotation=False; circular_padding=False"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
